<?xml version="1.0" encoding="UTF-8"?>
<services version="1.0" xmlns:deploy="vespa">

    <container id="default" version="1.0">
        <component id="openai" class="ai.vespa.llm.clients.OpenAI">
            <config name="ai.vespa.llm.clients.llm-client">
                <!-- <apiKeySecretName>...</apiKeySecretName> -->
            </config>
        </component>
        <component id="embedder" type="hugging-face-embedder">
            <transformer-model path="model/model.onnx"/>
            <tokenizer-model path="model/tokenizer.json"/>
            <prepend>
                <!--query>query:</query--> <!-- for now, this is added to queries elsewhere -->
                <document>passage: </document>
            </prepend>
            <onnx-intraop-threads>2</onnx-intraop-threads>
        </component>
        <component id="modernbert-embedder" type="hugging-face-embedder">
            <transformer-model model-id="nomic-ai-modernbert"/>
            <transformer-output>sentence_embedding</transformer-output>
            <pooling-strategy>none</pooling-strategy>
            <max-tokens>8192</max-tokens>
            <prepend>
                <query>search_query:</query>
                <document>search_document:</document>
            </prepend>
        </component>
        <document-processing>
            <chain id="default">
                <documentprocessor id="ai.vespa.cloud.docsearch.OutLinksDocumentProcessor" bundle="vespacloud-docsearch"/>
            </chain>
        </document-processing>
        <search>
            <chain id="default" inherits="vespa">
                <searcher id="ai.vespa.cloud.docsearch.DocumentationSearcher" bundle="vespacloud-docsearch"/>
            </chain>
            <chain id="suggest" inherits="vespa">
                <searcher id="ai.vespa.cloud.docsearch.SuggestionSearcher" bundle="vespacloud-docsearch"/>
            </chain>
            <chain id="llmsearch" inherits="vespa">
                <searcher id="ai.vespa.cloud.docsearch.LLMSearcher" bundle="vespacloud-docsearch"/>
            </chain>

            <chain id="slackmessage" inherits="vespa">
                <searcher id="ai.vespa.cloud.docsearch.SlackMessageSearcher" bundle="vespacloud-docsearch"/>
            </chain>
            <chain id="thread" inherits="vespa">
                <searcher id="ai.vespa.cloud.docsearch.ThreadSearcher" bundle="vespacloud-docsearch"/>
            </chain>

            <chain id="combinedllmsearch" inherits="vespa">
                <federation id="combination">
                    <source idref="thread"/>
                    <source idref="llmsearch" />
                </federation>
            </chain>


            <chain id="ragsearch" inherits="vespa">
                <searcher id="ai.vespa.search.llm.RAGSearcher">
                    <config name="ai.vespa.search.llm.llm-searcher">
                        <providerId>openai</providerId>
                        <promptTemplate>files/prompt.txt</promptTemplate>
                    </config>
                </searcher>
                <searcher id="ai.vespa.cloud.docsearch.LLMSearcher" bundle="vespacloud-docsearch"/>
            </chain>
            <chain id="combinedragsearch" inherits="vespa">
                <searcher id="ai.vespa.search.llm.RAGSearcher">
                    <config name="ai.vespa.search.llm.llm-searcher">
                        <providerId>openai</providerId>
                        <promptTemplate>files/prompt_combined.txt</promptTemplate>
                    </config>
                </searcher>
                <federation id="combinedragsearchsource">
                    <source idref="thread"/>
                    <source idref="llmsearch" />
                </federation>
            </chain>
        </search>
        <document-api/>
        <nodes count="2" exclusive="true">
            <resources vcpu="2" memory="8Gb" disk="32Gb" />
            <resources vcpu="4" memory="16Gb" disk="125Gb" deploy:instance="default" deploy:environment="prod" deploy:region="aws-us-east-1c">
                <gpu count="1" memory="16Gb"/>
            </resources>
            <resources vcpu="2" memory="8Gb" disk="32Gb" architecture="x86_64" deploy:instance="enclave" />
        </nodes>
    </container>

    <container id="playground" version="1.0">
        <handler id="ai.vespa.cloud.playground.TensorPlaygroundHandler" bundle="vespacloud-docsearch">
            <binding>http://*/playground/*</binding>
        </handler>
        <nodes count="2">
            <resources vcpu="2" memory="8Gb" disk="32Gb" />
            <resources vcpu="2" memory="8Gb" disk="32Gb" architecture="x86_64" deploy:instance="enclave" />
        </nodes>
    </container>

    <content id="documentation" version="1.0">
        <config name="vespa.config.search.summary.juniperrc">
            <length>1024</length> <!-- default 256 -->
            <min_length>512</min_length> <!-- default 128 -->
            <surround_max>512</surround_max> <!-- default 128 -->
            <max_matches>2</max_matches> <!-- default 3 -->
            <winsize>512</winsize> <!-- default 200 -->
        </config>
        <min-redundancy>2</min-redundancy>
        <documents>
            <document mode="index" type="doc"/>
            <document mode="index" type="paragraph"/>
            <document mode="index" type="term"/>
            <document mode="index" type="purchase"/>
            <document mode="index" type="slack_message"/>
            <document mode="index" type="code_snippet"/>
        </documents>
        <nodes count="2">
            <resources vcpu="2" memory="8Gb" disk="32Gb" architecture="x86_64" deploy:instance="enclave" />
            <resources vcpu="2" memory="8Gb" disk="118Gb" storage-type="local" deploy:instance="cloud-enclave" deploy:cloud="aws"/>
        </nodes>
    </content>

    <config name="ai.vespa.llm.clients.triton">
        <target>triton:8001</target>
        <modelRepositoryPath>/sidecars/triton/models</modelRepositoryPath>
    </config>
</services>
